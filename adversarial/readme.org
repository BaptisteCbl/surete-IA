

* Installation from fresh environment.
** Create the environment (might be slow):
#+begin_src bash
conda env create --file robust.yml
#+end_src

- If the installation is stuck on ~Solving environment: |~ try:
#+BEGIN_SRC bash
conda config --set channel_priority strict
#+END_SRC
- Revert with:
#+BEGIN_SRC bash
conda config --set channel_priority true
#+END_SRC
** Activate the environment:
#+begin_src bash
conda activate Robust
#+end_src
** Setup the project:
#+begin_src bash
pip install -e .
#+end_src
The scripts in ~adversarial/src/tf2/~ need some extra packages which can be installed using :
#+begin_src bash
pip install -e .['base, tf']
#+end_src

* Running the  scripts

Running the basic training from adversarial/
#+begin_src bash
python src/pytorch/basic/training.py  --flagfile=config_files/training.cfg
#+end_src


Running the fast adversarial training (for some parameters we keep the default value defined in the ~get_args~) function
#+begin_src bash
python src/pytorch/fast_gradAlign/train.py --model=cnn_small --dataset=CIFAR10 --attack=pgd --eps=10 --attack_init=zero --epochs=40 --eval_iter_freq=50 --lr_max=0.003 --gpu=0 --n_final_eval=1000
#+end_src

Running the evaluation
#+begin_src bash
python src/pytorch/evaluation.py  --flagfile=config_files/evaluation.cfg
#+end_src

** The configuration files

We use the /abseil/ library to pass parameters to the code with the ~FLAGS~ from a config file using the ~flagfile~ parameter when executing a python script.
We use two configuration files one for training and one for the evaluation. The display configuration file is the same as the evaluation one.
*** Basic training configuration file
- For the *model*
    - ~model~ the model's name which can be found in ~src/models~. The model's class name must be the name as the module name (e.g. cnn.py and class cnn()).
    - ~save~ the name to use when saving the model.
- For the *data*
    - ~data~ the dataset name. The dataset will be loaded from ~torchvision.datasets~ using only its name.
    - ~dim~ the height and width of the images.
    - ~batchsize~ the batch size to use for the loader.
- For the model's *hyperparameters*
    - ~nb_epoch~ the number of epochs needed for the training.
    - ~in_channels~ the number of inputs channels, it is usually the number of channels of the images (1 or 3).
    - ~out_channels~ the number of output channels, it is usually the number of classes to predict.

- For the *adversarial training*
    - ~adv_train~ boolean if the training must use adversarial examples.
    - ~eps~ the epsilon value used for computing the adversarial example.

*** Evaluation configuration file
- For the *model*
    - ~save~ the name of the save to load.
- For the *data*
    - ~data~ the dataset name. The dataset will be loaded from ~torchvision.datasets~ using only its name.
    - ~batchsize~ the batch size to use for the loader (a smaller batch size can help for ~torch.cuda.OutOfMemoryError~). 
- For the *attacks*   
    - ~attacks~ list of the attacks' name to use.
- For the attacks' *parameters*
    - ~eps~ list of the epsilon values to use for FGSM and PGD attacks.
    - ~norm~ the norm to use for the different attacks (0,1,2, np.inf).
    - ~clip_min~ minimal clip value for the image.
    - ~clip_max~ max clip value for the image either 1/255 for float/int values.
    - ~num_classes~ the number of classes in the dataset
    - ~<attack>_parameters~ a list of the /<attack>/ parameters. 

** The project code

- *models* are defined in ~src/pytorch/models~. 
- *saved* models are in ~src/pytorch/saves/<approach>~. 
- *configuration* files are in ~config_files~. 
- *data* are (locally) saved in ~data/~. 
-  *evaluation* and *display* scripts are in ~src/pytorch~.
- *attacks* are in ~src/attacks~ they come from [[https://github.com/cleverhans-lab/cleverhans][CleverHans]]
 and [[https://github.com/DSE-MSU/DeepRobust][DeepRobust]]. The attacks from *CleverHans* work out of the box, 
 the attacks from *DeepRobust* have been modified to take in inputs batch of images instead of a single image.
- There are 3 different approach to train a model: 
    - ~src/pytorch/basic/~ which is "basic" training loop working with all model/data pairs, it can be used to
    perfrom adversarial training
    - ~src/pytorch/fast~ based on [[https://github.com/locuslab/fast_adversarial][fast adversarial]] training approach. There are two folders one to work on MNIST/FashionMNIST and one to work on CIFAR10.
    - ~src/pytorch/fast_gradAlign~ based on another [[https://github.com/tml-epfl/understanding-fast-adv-training][fast adversarial]] training approach. It works on MNIST, FashionMNIST and CIFAR10.

** Logs 
In the log file we store the training loss/accuracy/time (and test in the gradAlign) by epoch.
The file are in .csv format using logging. The gnuplot script allows to quicly plot some data.

** Evaluation logs
It contains the logs of the different evaluation.

** Running the scripts in ~adversarial/src/tf2/~
The two experiments performed here are Confidence Learning and Parametric Noise Injection, these scripts were originally run on Colab Notebooks; the contents of ~adversarial/src/tf2/~ are a ~.py~ files version of these notebooks.
Everything happens in the ~main.py~ where the different hyperparameters and experiments to run can be chosen :
#+begin_src python
RUN_TYPE = "Confidence" #@param ["Confidence", "Noise"] {type:"raw"}
#+end_src
is for the experiment choice : ~Confidence~ for confidence learning, and ~Noise~ for Parametric Noise Injection;
#+begin_src python
EPOCHS = 20 #@param {type:"slider", min:0, max:20, step:1}
BATCH_SIZE = 128 #@param ["512", "256", "128"] {type:"raw"}
ADVERSARIAL_TRAINING = False #@param {type:"boolean"}
EPSILON = 0.3 #@param {type:"slider", min:0, max:1, step:0.1}
#+end_src
are the common parameters of the experiments : 
- ~EPOCHS~ is the number of epochs for training;
- ~BATCH_SIZE~ is the training batch size;
- ~ADVERSARIAL_TRAINING~ indicates if adversarial training is done  or not;
- ~EPSILON~ is the epsilon hyperparameter of the adversarial attacks (for FGSM and PGD);
#+begin_src python
LAMBDA = 2 #@param {type:"slider", min:0, max:3, step:0.1}
#+end_src
is a hyperparameter of the loss in the confidence learning experiment;
#+begin_src python
NOISY_MODEL = False #@param {type:"boolean"}
ADVERSARIAL_TRAINING_COEFFICIENT = 0.5 #@param {type:"slider", min:0, max:1, step:0.1}
#+end_src
these are parameters of the Parametric Noise Injection ; the former indicating if noise is actually injected in the model or not, and the latter parametrizing the percentage of adversarial examples seen by the model at each batch when ~ADVERSARIAL_TRAINING~ is set to ~True~.