#+TITLE: Adversarial Attacks and Defenses in Images, Graphs and Text: A Review
* STRT Notes
:PROPERTIES:
:Custom_ID: xuAdversarialAttacksDefenses2019
:NOTER_DOCUMENT: /home/guillaume/Zotero/storage/W9U9MNRW/Xu et al. - 2019 - Adversarial Attacks and Defenses in Images, Graphs.pdf
:AUTHOR: Xu, H. et al.
:JOURNAL:
:DATE: 2019-10-09
:YEAR: 2019
:DOI:  http://dx.doi.org/10.48550/arXiv.1909.08072
:URL: http://arxiv.org/abs/1909.08072
:END:
#+SETUPFILE: ./setup/setup.org

#+begin_quote
Adversarial examples are inputs to machine learning models that an attacker intentionally designed to cause the model to make mistakes
#+end_quote

* Abstract
:PROPERTIES:
:NOTER_PAGE: 1
:END:
Survey, in the context of adversarial examples, of attacks and defense machanisms for DNN models on images, graphs and texts.
* Introduction
:PROPERTIES:
:NOTER_PAGE: 1
:END:

* Definitions and Notations
:PROPERTIES:
:NOTER_PAGE: 2
:END:
** Threat Model
:PROPERTIES:
:NOTER_PAGE: 2
:END:
*** Adversary's goal
:PROPERTIES:
:NOTER_PAGE: 2
:END:
Purpose:
- misguide classifier on 1 sample
- influence overall performance

- Poisonning attacks:
  insert/modify several fake samples into training database.
- Evasion attacks:
  craft some fakes that the classifier cannot recognize. Generate fraudulent sample that evade detection by the classifier.

- Targeted attacks:
  victim sample $(x,y)$ $x$ feature vector, $y \in upsilon$ ground truth. The adversary aims to induce the classifier to give the label $t \in \upsilon$ for the perturbated sample $x'$.
- Non-targeted attacks:
  The target label $t$ is not specified. The adversary only wants the classifierto predict incorrectly.

*** Adversary's knowleged
:PROPERTIES:
:NOTER_PAGE: 3
:END:
Knowledge:
- structure
- parameters
- training set

- White-box attack
  access to all information of the target NN : architecture, parameters, gradients...
  Security  against WB attacks is what is desired.

- Black-box attack:
  inner configuration not available. The adversary can only choose the input and observe the output.

- Semi-white Box attack:
  the adversary trains a generative model to produce adversarial examples in a white-box setting
*** Victim models
:PROPERTIES:
:NOTER_PAGE: 3
:END:
Which models and why to attack
**** Conventional Machine Learning Models
:PROPERTIES:
:NOTER_PAGE: 3
:END:
SVM, fully-connected shallow NN, Bayesian
**** Deep Neural Networks
:PROPERTIES:
:NOTER_PAGE: 3
:END:
***** Fully-Connected Neural Networks
:PROPERTIES:
:NOTER_PAGE: 3
:END:

***** Convolutional Neural Networs
:PROPERTIES:
:NOTER_PAGE: 3
:END:
***** Graph Convolutional Networks
:PROPERTIES:
:NOTER_PAGE: 3
:END:
***** Recurrent Neural Networks
:PROPERTIES:
:NOTER_PAGE: 4
:END:
*** Robusteness
:PROPERTIES:
:NOTER_PAGE: 4
:END:
Here $\lVert .\rVert$ usually refers to $l_p$ norm.
#+ATTR_LATEX: :options [Minimal perturbation]
#+begin_definition Minimal perturbation
F a classifier, $(x, y)$ the data, the adversarial perturbation has the least norm (most unnoticeable perturbation)
$$\delta_{min} = \underset{\delta}{\operatorname{arg min }} \lVert \delta \rVert$$
#+end_definition

#+ATTR_LATEX: :options [Robusteness]
#+begin_definition
The norm of minimal perturbation:
$$r(x,F) = \lVert \delta_{min} \rVert$$
#+end_definition

#+ATTR_LATEX: :options [Global robusteness]
#+begin_definition
The expectation of robusteness over the whole population D:
$$\rho(F) = \underset{x\sim D}{\mathbb{E}} r(x,F)$$
#+end_definition
 Minimal perturbation can find the adversarial example most similar to $x$ under the model $F$. The larger $r(x,F)$ or $\rho(F)$ is, the more the adversary needs to sacrifice similarity to generate a sample. It implies that $F$ is robust.
*** Adversarial Risk (Loss)
:PROPERTIES:
:NOTER_PAGE: #+startup: num4
:END:
#+ATTR_LATEX: :options [Most-adversarial example]
#+begin_definition
A classifier F and data x, the sample $x_{adv}$ with the largest loss
value in $x$â€™s \varepsilon-neighbor ball:
$$x_{adv} = \underset{\delta}{\operatorname{arg max }}~ \mathcal{L}(x',F)$$
Subjet to
$\lVert x' - x \rVert \leq \varepsilon$
#+end_definition

#+ATTR_LATEX: :options [Adversarial loss]
#+begin_definition
The loss value for the most-adversarial example:
$$\mathcal{L}_{adv}(x) =  \mathcal{L}(x_{adv}) = \underset{\lVert x' - x \rVert \leq \varepsilon}{\operatorname{max }}~ \mathcal{L}(\theta,x',y)$$
Subjet to
$\lVert x' - x \rVert \leq \varepsilon$
#+end_definition


#+ATTR_LATEX: :options [Global adversarial loss]
#+begin_definition
The expectation of the loss value on $x_{adv}$ over the data distribution $D$:
$$\mathcal{R}(F) = \underset{x\sim D}{\mathbb{E}} \underset{\lVert x' - x \rVert \leq \varepsilon}{\operatorname{max }}~ \mathcal{L}(\theta,x',y)$$
#+end_definition

The most-adversial example is the point whehre the model is most likelt to be fooled in the neighborhood of x. A lower loss value of $\mathcal{L}_{adv}$ indicates a more robust classifier.

* Generating Adversarial Examples

:PROPERTIES:
:NOTER_PAGE: 4
:END:
** White-box Attacks
:PROPERTIES:
:NOTER_PAGE: 5
:END:
*** Biggio's Attack
:PROPERTIES:
:NOTER_PAGE: 5
:END:
SVM and 3-layer FCNN. Optimize the discriminant function
*** Szegedy's L-BFGS Attack
:PROPERTIES:
:NOTER_PAGE: 5
:END:
<<algo:LBFGS>>
DNN image classifier. Minimize  $\lVert x - x'\rVert_{2}^{2}$, st $C(x')=t$ which is transform to a penalisation term to enforce the Classier to predict $x'$ as $t$, parameter $c$ allow to find minimal distance. Solve with L-BFGS.

$$c\lVert x - x'\rVert_{2}^{2} + \mathcal{L}(\theta, x', t)$$
St $x' \in [O,1]^m$

*** Fast Gradient Sign Method
:PROPERTIES:
:NOTER_PAGE: 5
:END:
<<algo:FGSM>>
$$x' = x + \varepsilon \operatorname{sign}(\nabla_x  \mathcal{L}(\theta, x,y)) \text{ non-target}$$
$$x' = x - \varepsilon \operatorname{sign}(\nabla_x  \mathcal{L}(\theta, x, t)) \text{ target t}$$

$min \matcal{L}(\theta, x', t)$
St $\lVert x' - x \rVert _{\infty}$ and $x' \in [O,1]^m$

*** Deep Fool
:PROPERTIES:
:NOTER_PAGE: 6
:END:
Decision boundary: hyperplane => polyhedron => minimal perturbation projection.

*** Jacobian-Based Saliency Map Attack
:PROPERTIES:
:NOTER_PAGE: 6
:END:
Jacobian of the score function $F$. Can be see as greedy attack algo by iteratively manipulating the pixel which is the most influential to the model output.
$$J_{F}(x) = \frac{\partial F(x)}{\partial x}$$ to model $F(x)$'s change in response to change of its input $x$.
Can be targeted.
*** Basic Iterative Method (BIM)/Projected Gradient Descent (PGD) ATTACK
:PROPERTIES:
:NOTER_PAGE: 6
:END:
Iterative version of [[algo:FGSM][FGSM]]. Non targeted setting:
$x':$
$$x_{0} = x$$
$$x^{t+1} = \operatorname{Clip}_{x,\varepsilon} (x^{t} + \alpha \operatorname{sign}(\nabla_x  \mathcal{L}(\theta, x,y)))  $$

$\operatorname{Clip}$ function to project it argument to the surface of $x$ $\varepsilon$ neighbor ball.
$\alpha$ step size

Search the samples $x'$ which have the largest loss value in the $l_{\infty}$ ball around $x$. These samples are called "most-adversarial" examplen most aggresive and most-likely to fool the classifiers whent perturbation intensity is limited.

First work to seriously calculate the exact robusteness however SMT solver slow and not scalable.
*** Carlini & Wagner's Attack
:PROPERTIES:
:NOTER_PAGE: 7
:END:
Reformulate problem in [[algo:LBFGS][L-BFGS]] by minimizing
$$\lVert x - x' \rVert _{2}^{2} + c\cdot f(x',t) $$
St $x' \in [O,1]^m$
with $f(x',t) = \operatorname{max} Z(x')_{i} - Z(x')_{t}. i \neq t$.

Minimizing $f$ encourages the algorithm to find an $x'$ that has larger score for class $t$. By applying line search on constant $c$ we can find the $x'$ that has the least distance to $x$.

$f$ can be called margin loss function.

The only difference with [[algo:LBFGS][L-BFGS]] is the use of margin loss instead of corss entropy loss. The advantage is that when $C(x') = t$, $f(x',t) = 0$, the algorithm will directly minimize the distance from $x'$ to $x$
*** Ground Truth Attack
:PROPERTIES:
:NOTER_PAGE: 7
:END:
"Provable strongest attack". Method to find the theoretical minimmally-distroted adversarial example.

Based on [[https://link.springer.com/chapter/10.1007/978-3-319-63387-9_5][Reluplex]] (algo for verifying the properties of NN). It encodes the model parameters $F$ and data $(x,y)$ as the subjects of linear-like programming system. Then solves the system to check whether there exists an eligible sample $x'$ in $x$'s neighbor that can fool the model. The can be reducing the radius of the ball, the last example found is the grond truth adversarial attack because it has been proved to have least dissimilarity with $x$.
*** Other lp Attack
:PROPERTIES:
:NOTER_PAGE: 7
:END:
- One-pixel attack, $l_{0}$ norm limit the number of pixel that are allowed to be changed
- EAD: /Elastic-Net Attack/, $l_{1}$, $l_{2}$ norm together. Some model are good against $l_{infty}$ and $l_{2}$ but still vulnerable to $l_{1}$ based EN attack.
*** Universal Attack
:PROPERTIES:
:NOTER_PAGE: 8
:END:
Find a perturbation $\delta$ satisfying:
$\lVert \delta \rVert _{p} \leq \varepsilon$
$$ \underset{x\sim D(x)}{\mathbb{P}} (C(x+\delta) \neq C(x)) \leq 1 - \sigma $$


Find perturbation st the classifier gives wrong answer for most of the samples.

*** Spatially Transformed Attack
:PROPERTIES:
:NOTER_PAGE: 8
:END:
Rotate, translate, distort the local image features sightly.
*** Unrestriceted Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 8
:END:
Do not look exactly the same as the victim samples but still legitimate
** Physical World Attack
:PROPERTIES:
:NOTER_PAGE: 8
:END:
*** Exploring Adversarial Examples in Physical World
:PROPERTIES:
:NOTER_PAGE: 8
:END:
Robust under natural transformation (viewpoint, lighting, ...)
*** Eykholt's Attack on Road Signs
:PROPERTIES:
:NOTER_PAGE: 9
:END:
Sticker on the stop sign in the desired position
** Black-Box Attacks
:PROPERTIES:
:NOTER_PAGE: 9
:END:
*** Subsitute Model
:PROPERTIES:
:NOTER_PAGE: 9
:END:
Exploit "transferability": a sample $x'$ can attack $F_{1}$ is also likely to attack $F_{2}$. Method to train substitute model.
1) Synthetise Substitue Training Dataset
2) Train the Substitute model
3) Augment Dataset
4) Attack the substitute model

Choose attack with high "transferability" as FGSM, PGD
*** ZOO: Zeroth Order Optimization Based Black-Box Attack
:PROPERTIES:
:NOTER_PAGE: 10
:END:
*** Query-Efficient Black-Box Attack
:PROPERTIES:
:NOTER_PAGE: 10
:END:
** Semi-whithe (Grey) box Attack
:PROPERTIES:
:NOTER_PAGE: 10
:END:
Train a GAN targeting the model of interest. The attacker can craft adversarial example from the GAN. Benefits: it accelerates the process of producing
** Poisonning attacks
:PROPERTIES:
:NOTER_PAGE: 10
:END:
*** Biggio's Poisonning Attack on SVM
:PROPERTIES:
:NOTER_PAGE: 10
:END:
Poison sample, use incremental learning techniques for SVM.
*** Koh's Model Explanation
:PROPERTIES:
:NOTER_PAGE: 10
:END:
Method to interpret DNN. Model can explicity quantify the change in the final loss without retraining the model when only one training is modified. This work can be adapted to poisonning attacks by finding those training samples that have large influence on model's prediction.
*** Poison Frogs
:PROPERTIES:
:NOTER_PAGE: 10
:END:
Insert adversarial image with true label to the training set in order to cause the trained model to wrongly classify a targeted sample.
* Countermeasures Against Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 11
:END:
** Gradient Masking/Obfuscation
:PROPERTIES:
:NOTER_PAGE: 11
:END:
*** Defensive Distillation
:PROPERTIES:
:NOTER_PAGE: 11
:END:
"Distillation" is a technique to reduce the size of DNN architectures.
Reformulate the procedure to train a model that can resist to adversarial attack
1) Train a network $F$ on the given training set $(X,Y)$ by setting the temperature of the softmax to $T$.
2) Compute the scores given by $F(X)$ again and evaluate the scores at temperature $T$
3) Train another network $F'_{T}$,/the distilled model/, using softmax at temperature Â£T$ on the dataset with soft labels $(X, F(X))$.
4) Use the distilled network with softmax at temperature $T=1$, which is denoted $F'_{1}$ during prediction on test data (or adversarial example)

We cause the inputs to the softmax to become larger by a factor of $T$, meaning for a sample $x$ its neighbor $x'$ will be 100 times larger.
*** Shatered Gradients
:PROPERTIES:
:NOTER_PAGE: 11
:END:
Protect the model by prerpocessing the data. Non-smooth or non-differentiable prerpocessor $g(.)$ and then train a DNN model $f$ on $g(X)$. The train classifier on $f(g(X))$ is not differentiable in term of $x$, causing the failure of adversarial attacks.
/thermometer encoding/ uses a preprocessor to discretize an image's pixel value $x_{i}$ into a $l$-dimensional vector.
*** Stochastic/Randomized Gradients
:PROPERTIES:
:NOTER_PAGE: 12
:END:
Randomize the DNN model in order to confound the adversary. We train a set of classifiers $s={F_{t}: t = 1,2,...,k}}$. During evaluation on data $x$, we randomly select one classifier from the set $s$ and predict the label $y$.

As the adversary has no idea which classifier is used by the prediction model, the attack sucess rate will be reduced.
Dropout can be added.
*** Exploding & Vanishing Gradients
:PROPERTIES:
:NOTER_PAGE: 12
:END:
Generative models to project a potential adversarial example onto the benign data manifold before vlassifying them. Add a generative model before the classifier DNN which cause the fianl classification model to be extremely DNN. The cumulative product of partial derivatives from each layer will cause the gradient to be extremelly small or irrgularly large, which prevents the attacker accurately estimation the location of adversarial examples.

*** Gradient Masking/Obfuscation are not safe
:PROPERTIES:
:NOTER_PAGE: 12
:END:
** Robust Optimization
:PROPERTIES:
:NOTER_PAGE: 12
:END:
Aim to improve the classifier's robustness by changing manner of learning. Major focus:
1) minnimize the average adversarial loss
2) Maximize the average minimal perturbation distance

   Typically, a robust optimization algorithm should have a prior knowledge of its potential threat or potential attack. The the defenders build classifiers which are safe against this specific attack.
*** Regularization Methods
:PROPERTIES:
:NOTER_PAGE: 12
:END:
1. Penalize Layers'Lipschitz Constant:
   can force the trained model to be stable. Constraining the Lipschitz constant $L_{k}$ between any two layers.

   $$\forall~ x,~ ~ \delta \lVert h_{k}(x;W_{k}) - h_{k}(x+\delta;W_{k}) \rVert \leq L_{k} \lVert \delta \rVert$$

   The outcome of each layer will not be easily influenced by the small distorsion of its input.

   Generalisation, during the training penalizing the large instability for each hidden layer can help to decrease the adversarial risk of the model and consequently increase tge robustness.

   $$ \underset{x\sim D}{\mathbb{E}} \mathcal{L}_{adv}(x) \leq \underset{x\sim D}{\mathbb{E}} \mathcal{L}(x) + \lambda_{p} \prod_{k=1}^{K} L_{k} $$
   with $\lambda_{k}$ the Lipschitz constant of the loss function.

2. Penalize Layer's Partial derivative
   Deep Contractive Network algorithm to regularize the training. It suggests adding a penalty on the partial derivatives at each layer into the standard back-propagation. So that the change of the input data will not cause large change on the output of each layer.
   Thus, it becomes difficult fot the classifier to give different predictions on perturbated data samples.

*** Adversarial (Re)Training
:PROPERTIES:
:NOTER_PAGE: 13
:END:
a) Adversarial training with FGSM
   Non-targeted [[algo:FGSM][FGSM]] to generate adversarial example $x'$ for the training dataset.
   Increase the robusteness against FGSM attack.

   Improved for scalability with batch and batch normalization

b) Adversarial training with PGD
   Projected gradient descent attack. PGD attack can be seen as a heuristic method to find the "most adversarial" example in th e $l_{\infty}$ ball around $x$. It solves the problem if learning model parameters $\theta$ that minimize the adversarial loss.
   Method training only on adversarial examples, instead of a mix of benign and adversarial examples.

   Good robusteness against both single-step and iterative attack on MNIST and CIFAR10. However the training is hard to scale as the method involves an iterative attack for all training samples.

c) Ensemble adversarial training
   Protect CNN against single-step attack and can be applied to large datasets such as ImageNet.

   Augment the classifier's,$F$ training set with adversarial exemple crafted from other pre-trained classifiers $F_{1},F_{2},\dots$ . Then, for each sample $x$, we use a single-step attack [[algo:FGSM][FGSM]] to craft adversarial examples on the other classifier. Beacause of the transferability of the single-step attack across different model are also likely to mislead the classifier $F$. It means that thes samples are a good approximation for "the most adversarial" example for model $F$ on $x$. Training these samples together will approximately minimize the adversarial loss.

d) Accelerate adversarial training
   Free adversarial training which improves the efficiency by reusong the backward pass calculation.
   The gradient of the loss to input: $\frac{\partial \mathcal{L}(x+\delta, \theta)}{\partial x}$ and the gradient of the loss to model parameters: $\frac{\partial \mathcal{L}(x+\delta, \theta)}{\partial \theta}$ can be computed together in one back propagation. (You Only Propagate Once (YOPO))


*** Provable Defenses
:PROPERTIES:
:NOTER_PAGE: 14
:END:


** Adversarial Example Detection
:PROPERTIES:
:NOTER_PAGE: 15
:END:
*** An Auxiliary Model To Classify Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 16
:END:
**** Using Statistics To Distinguish Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 16
:END:
**** Checking The Prediction Consistency
:PROPERTIES:
:NOTER_PAGE: 16
:END:
**** Some Attacks Which Evade Adversarial Detection
:PROPERTIES:
:NOTER_PAGE: 16
:END:
* Explanations for the Existence of Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 17
:END:
** Why Do Adversarial Examples Exist?
:PROPERTIES:
:NOTER_PAGE: 17
:END:
** Can We Build an Optimal Classifier
:PROPERTIES:
:NOTER_PAGE: 17
:END:
** What is Transderability?
:PROPERTIES:
:NOTER_PAGE: 17
:END:
* Graph Adversarial Examples
:PROPERTIES:
:NOTER_PAGE: 17
:END:
** Definitions for Graphs and Graph Models
:PROPERTIES:
:NOTER_PAGE: 18
:END:
** Zugner's Greedy Method
:PROPERTIES:
:NOTER_PAGE: 18
:END:
** Dai's RL Method: RL-S2V
:PROPERTIES:
:NOTER_PAGE: 18
:END:
** Graph Structure Poisoning via Meta-Learning
:PROPERTIES:
:NOTER_PAGE: 19
:END:
** Attack on Node Embedding
:PROPERTIES:
:NOTER_PAGE: 19
:END:
** ReWatt: Attacking Graph Classifier via Rewiring
:PROPERTIES:
:NOTER_PAGE: 19
:END:
** Defending Graph Neural Networks
:PROPERTIES:
:NOTER_PAGE: 19
:END:
* Adversarial Examplesin Audio and Text Data
:PROPERTIES:
:NOTER_PAGE: 19
:END:
* Adversarial Examples in Miscellaneous Tasks
:PROPERTIES:
:NOTER_PAGE: 21
:END:
** FComputer Vision Beyond Image Classification
:PROPERTIES:
:NOTER_PAGE: 22
:END:
* Conclusion
:PROPERTIES:
:NOTER_PAGE: 23
:END:


